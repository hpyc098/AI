import os
import json
import requests
from flask import Flask, render_template, request, jsonify
from bs4 import BeautifulSoup
import urllib.parse
import time
import random

# ========== å¯é€‰ç¿»è¯‘åº“ ==========
try:
    from deep_translator import GoogleTranslator
    TRANSLATE_ENABLED = True
except ImportError:
    print("âš ï¸ deep_translator æœªå®‰è£…ï¼Œè”ç½‘æŠ“å–å†…å®¹å°†ä¸ä¼šç¿»è¯‘")
    TRANSLATE_ENABLED = False

# ========== åŸºæœ¬é…ç½® ==========
app = Flask(__name__, template_folder="templates")

PASSWORD = "Hjh20131121"
KB_FILE = "knowledge_base.json"
MAX_PAGES = 5
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/115.0 Safari/537.36"
)

# ========== çŸ¥è¯†åº“å­˜å– ==========
def load_kb():
    if os.path.exists(KB_FILE):
        with open(KB_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_kb(kb):
    with open(KB_FILE, "w", encoding="utf-8") as f:
        json.dump(kb, f, ensure_ascii=False, indent=2)

# ========== æœç´¢ä¸æŠ“å– ==========
def extract_real_url(ddg_url):
    if "uddg=" in ddg_url:
        parsed = urllib.parse.urlparse(ddg_url)
        query = urllib.parse.parse_qs(parsed.query)
        real_url = query.get("uddg", [""])[0]
        return urllib.parse.unquote(real_url)
    return ddg_url

def search_urls(query, max_results=MAX_PAGES):
    search_url = f"https://duckduckgo.com/html/?q={urllib.parse.quote(query)}"
    try:
        resp = requests.get(search_url, headers={"User-Agent": USER_AGENT}, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        links = []
        seen = set()
        for a in soup.find_all("a", class_="result__a", href=True):
            url = extract_real_url(a["href"])
            if url and url not in seen:
                seen.add(url)
                links.append(url)
                if len(links) >= max_results:
                    break
        return links
    except Exception as e:
        print(f"æœç´¢å¤±è´¥: {e}")
        return []

def scrape_page(url):
    """æŠ“å–ç½‘é¡µæ­£æ–‡å¹¶è‡ªåŠ¨ç¿»è¯‘è‹±æ–‡"""
    try:
        print(f"ğŸ“– æŠ“å–å†…å®¹ï¼š{url}")
        resp = requests.get(url, headers={"User-Agent": USER_AGENT}, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")
        paragraphs = [p.get_text(strip=True) for p in soup.find_all("p")]
        content = "\n".join(paragraphs)
        if not content:
            content = soup.get_text(separator="\n", strip=True)[:2000]

        # è‡ªåŠ¨ç¿»è¯‘è‹±æ–‡åˆ°ä¸­æ–‡
        if TRANSLATE_ENABLED and content:
            try:
                translated = GoogleTranslator(source='auto', target='zh-CN').translate(content)
                return translated
            except Exception as e:
                print(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
                return content

        return content
    except Exception as e:
        print(f"æŠ“å–å¤±è´¥ {url}: {e}")
        return ""

def search_online(query):
    print(f"ğŸ” æ­£åœ¨è”ç½‘æœç´¢ï¼š{query}")
    urls = search_urls(query, max_results=MAX_PAGES)
    results = []
    for url in urls:
        text = scrape_page(url)
        if text:
            results.append(text)
        time.sleep(random.uniform(0.5,1.2))

    if not results:
        return "æœªèƒ½ä»ç½‘ä¸Šè·å–åˆ°æœ‰æ•ˆå†…å®¹ã€‚"

    return "\n\n".join(results[:3])  # å‰3æ¡å†…å®¹

# ========== é¡µé¢ ==========
@app.route("/")
def index():
    return render_template("index.html")

@app.route("/admin")
def admin():
    return render_template("admin.html")

# ========== èŠå¤©æ¥å£ ==========
@app.route("/api/chat", methods=["POST"])
def chat():
    data = request.get_json()
    user_input = data.get("message", "").strip()
    if not user_input:
        return jsonify({"reply": "âŒ è¯·è¾“å…¥é—®é¢˜ã€‚"})

    kb = load_kb()
    for item in kb:
        if user_input in item.get("question", ""):
            return jsonify({"reply": f"[çŸ¥è¯†åº“å›ç­”]\n{item.get('answer')}"})

    online_answer = search_online(user_input)

    kb.append({
        "question": user_input,
        "answer": online_answer,
        "time": time.strftime("%Y-%m-%d %H:%M:%S")
    })
    save_kb(kb)

    return jsonify({"reply": f"[è”ç½‘æœç´¢ç»“æœ]\n{online_answer}"})

# ========== åå°æ•™å­¦æ¥å£ ==========
@app.route("/api/admin", methods=["POST"])
def admin_api():
    data = request.json
    password = data.get("password", "")
    if password != PASSWORD:
        return jsonify({"reply": "âŒ å¯†ç é”™è¯¯"})

    action = data.get("action", "").strip()
    kb = load_kb()

    if action == "fetch_page":
        url = data.get("url", "").strip()
        if not url:
            return jsonify({"reply": "âŒ URL ä¸èƒ½ä¸ºç©º"})
        text = scrape_page(url)
        if not text:
            return jsonify({"reply": "âŒ æŠ“å–ç½‘é¡µå¤±è´¥"})
        kb.append({
            "question": url,
            "answer": text,
            "time": time.strftime("%Y-%m-%d %H:%M:%S")
        })
        save_kb(kb)
        return jsonify({"reply": f"âœ… å·²æŠ“å–ç½‘é¡µå¹¶åŠ å…¥çŸ¥è¯†åº“\n{text[:500]}..."})

    elif action == "teach":
        question = data.get("question", "").strip()
        answer = data.get("answer", "").strip()
        if not question or not answer:
            return jsonify({"reply": "âŒ é—®é¢˜æˆ–ç­”æ¡ˆä¸èƒ½ä¸ºç©º"})
        kb.append({
            "question": question,
            "answer": answer,
            "time": time.strftime("%Y-%m-%d %H:%M:%S")
        })
        save_kb(kb)
        return jsonify({"reply": f"âœ… å·²å­¦ä¹ é—®é¢˜ï¼š{question}"})
    else:
        return jsonify({"reply": "âŒ æœªçŸ¥ action ç±»å‹"})

# ========== å¯åŠ¨ ==========
if __name__ == "__main__":
    app.run(debug=True, port=5002, host="0.0.0.0")